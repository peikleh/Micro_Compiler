
\documentclass[12pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
%\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
%SetFonts

%SetFonts


\title{Crafting a Compiler}
\author{Nevin Leh\\Aaron Newhall\\Tim Weghen}
%\date{}							% Activate to display a given date or no date

\begin{document}
\begin{titlepage}
	\maketitle
\end{titlepage}
\setcounter{page}{2}
\tableofcontents
\section{Introduction}
The purpose of this project is to create a compiler. This is essentially a translation of a human readable language into a computer 
readable language. This is being done to explore the realm of computing at the most basic level and to illustrate the process of 
creating a compiler from start to finish.

When completing a degree in nearly any field it is necessary to complete a
course known as a capstone course. In this course, especially in fields such as
engineering, undergraduates get in groups and work on a project that lasts an
entire semester. The idea of these projects is basically a culmination of as many
of the main topics a student has learned in route to completing their degree as
possible. In computer science there are not many ideal projects that could be
chosen to highlight everything a CS student learns during their studies. However,
the one that was chosen for Montana State University computer science students
was to get into groups and create a Compiler. This project serves as a great
opportunity to not only show the department what their graduates are learning,
but it also serves to show the graduates themselves what they are capable of.
This idea of can roughly be thought of as a motivation for the project and class as
a whole

The object of this project was to create a compiler that converted a simple language called Micro into an assembly language called Tiny. These languages are fairly simple languages created so that creating a
compiler that worked for them would take the right amount of time. If the language
were too complicated the project would have been much too difficult to complete
in the given amount of time. The students were given the language and small test
programs, the compiler would then be designed to take in these programs in their
raw form, exactly as a programmer would have written them, break them down
into an intermediate language, then finally output a form broken down into a basic
machine language so the computer could actually perform the intended function
of the given code. Through lots of hard work and time spent, our group has
succeeded in creating a functioning compiler for the language Tiny that was
given to us.

This project was separated into four main steps each step building upon
the last in order to get us closer to the ultimate goal, these steps included:
Scanner, Parser, Symbol Table, and Semantic Routines. Each of these steps
was accompanied by not only hours and hours of class time learning the theories
and application behind each, but also mandatory labs created by the instructor to
give us hands on experience with the ideas behind the given step. This Report
will serve as a walk through of the entire process of creating a functioning
compiler. First, compilers in general will be explained including a brief history of
compilers with some basic information. This information will detail what exactly a
compiler does, meaning what is its purpose, along with a more technical look into
the different parts and where they fit in and what they actually do. Also, included
will be some information on real world example of compilers.

Next, the report will go into great detail of the process that our individual
group went through during each step of the project. In each section the process
of the given step is discussed including the method we used and how we
implemented the given step, as well as the many difficulties we encountered
along the way and how we got passed them. Also, the compiler as a whole will
be looked at including the process as a whole including tools used and how we
combined all the parts to create the final product. The final section of the report
will be a conclusion that details how the final product could be improved upon
given more time and resources. These improvements will be presented as a plan
and will cover topics such as optimization, including run-time memory
management as well as code optimizations. However, In order to fully understand
this project, as well as how it could be improved upon, and what exactly the
purpose of this capstone project is, one must learn what exactly a compiler is.
\section{Background}
Long before the modern era of programming languages programming was
much different. Instead of writing elaborate programs using development
environments with built in tools and libraries programmers had to
write code in languages such as assembly which is essentially machine code.
This style coding was much less efficient than writing programs in a high-level
language, which could be reused on different systems. However, early compilers
didn't always function better than hand written assembly and way many times
hindered by the lack of system memory.

The term compiler actually existed years before a complete compiler was
even created, though it wasn’t widely used, allegedly given the name by well
known computer scientist Grace Hopper while working on the A-O System
language. The “compiler” she was using was far from what comes to mind when
talking about compilers in the modern era. During the 50’s there were many other
scientists toying with the idea of compilers, but it wasn’t until 1957 that the first
complete compiler was born, created at IBM by the FORTRAN team lead by
John Backus. Work on compilers throughout the ‘60s paved the way for many of
ideas essential to our modern compilers. Early compilers were written mostly in
assembly, but by the 70’s compilers for languages such as C or Pascal were
written in their own languages. In the current era there are almost too many
different compilers to count. Each programming language has several, and more
and more are being created all the time.\cite{history}

So, what is a compiler? In a very general sense, a compiler is a computer
program that translates a computer program written in a given language, called a
source language, into an equivalent program of a different language, this
language is usually referred to as the target or output language. Though this may
seem like a pointless act, it actually serves a purpose. A compiler typically takes
a program written in a high level programming language (such as Java, C++,
Python, etc) and breaks it down into a form know as object code or machine code
that can be easily executed by a computer of virtual machine. Though compiling
from a high level language to a low level language isn’t always the case, it is
the most likely case. There are also compilers made to change one high level
language into another high-level language or even a low level language to a high
level. Though there may be many different kinds of compilers, one thing is for
sure, any programs written in a high-level programming language must be
translated to object code before it can be executed.

Most modern compilers are laid out in a similar way, meaning they have
the same basic parts and function in a similar way. Due to system restrictions
early compilers were divided in to multiple passes, each pass being a “run”
through the source code, gradually building up the internal data in the compiler.
Modern compilers like the one being created for this assignment work in two
stages, a front-end and a back-end. The front-end translates the code into an
intermediate representation, and the back-end produces the output language.
This project split up the process into four main steps: the Scanner, Parser,
Symbol Table, and Semantic Routines. Each of these steps play a pivotal role in
the compiler and without and one of them the process would not be possible. The
first step of the process when a written program comes in contact with the
compiler is the scanner, sometimes referred to a tokenizer or lexical analysis.
The Scanners is responsible for break up the input stream of the source code
into individual tokens or terminals, identifying the token type, and passing tokens
one at a time to the next in the process. These tokens are defined by the creator
of the compiler and can include items such as keywords, identifiers, or symbol
names. The scanner can be compared to the dictionary in human language in that it keeps the list of tokens(words) and their syntax(token type).

The next step in the process is called the parser. The parser contains
rules of the given language, and it is in this step that many syntax
errors are caught. These rules are usually referred to as a grammar. The parser focuses on the structure of the program identifying
the order of the tokens using a structure called a parse tree or abstract syntax
tree. The parser combined with the scanner make up the front-end portion of the
compiler.\cite{history}

\begin{figure}[h!]
	\centerline{\includegraphics[width=.4\linewidth]{pics/p_tree}}
	\caption{Parse tree for an English sentence\cite{pic}}
\end{figure}

 The third step of the process has to do with what is referred to as the
symbol table. The symbol table is an important step in the process because it is
actually a data structure created and altered by the compiler in order to store
information about the occurrences of items such as variable names and their
declarations as well as scope. The symbol table stores the names of all the
entities in a structured form to be used in the last step of the compiler.

The final step of the compiler can be referred to as the Semantic
routines. It is during this step that the main function of the compiler is completed.
During this step the compiler uses the symbol table generated by the front-end of
the compiler and generates an intermediate representation of the high-level
language program that went in. The compiler then translates that IR code into its
last step of compilation, which is the low level machine code ready to be
executed by the computer or virtual machine.

\begin{figure}[h!]
	\centerline{\includegraphics[width=.4\linewidth]{pics/diagram}}
	\caption{Components of the typical compiler}
\end{figure}


\section{Methods and Discussion}
\subsection{Scanner}
\subsubsection{Tools}
The lexer tool lex, provided by the PLY package for python, was used to create the scanner. This tool was chosen for it's simplicity and ease of use in creating a scanner. It is very similar to the lex program provided with most unix/linux systems and has a relatively easy setup compared to other scanner generators such as ANTLR. 
Initially ANTLR was the chosen scanner, but the very specific dependencies proved to be too complicated and a switch to PLY was decided upon. 

\subsubsection{Implementation}
Implementing the scanner with the help of PLY is a pretty trivial process. The guide
provided by the creators of PLY proved to be invaluable and very effective at
introducing the basics of creating a scanner.\cite{PLY} First, all of the token types have to be defined at the start of the scanner as a
list. These token types are used to define different types of valid tokens. These token types are necessary because the parser in the next step needs to be able to assemble the tokens into a parse tree. For the micro  grammar the token types IDENTIFIER, FLOATLITERAL, STRINGLITERAL, COMMENT, KEYWORD, and OPERATOR were defined.

The second step was to define regular expressions that match tokens to their token types. Regular expressions are used to define patterns within strings and have numerous uses. Each token type has a regular expression that defines the strings that need to match with that token type. The scanner then takes a Micro program as input and constantly takes
the first matching token and saves that token value along with its type in a list. If a part of the string doesn't get matched with a token type there is an error and the scanner stops and lets the user know that an invalid token has been
matched. If the list doesn't have any errors it will be passed to the parser to have the grammar of the program analyzed. 

The following code shows a very simple implementation of scanner that has one token type KEYWORD that matches the strings "START" and "END .
\begin{lstlisting}
	tokens = ('KEYWORD')
	t_KEYWORD = (r"BEGIN|END")
\end{lstlisting}
The syntax of defining tokens is very important as the PLY package uses the syntax to make a program that takes the program as input and returns a list of tokens and token types.

For the most part the rest of the leg work for creating the list of tokens is handled internally be the lexer.py file provided by PLY. One exception to this is that the scanner needs to ignore certain characters and lines such as spaces, tabs, and comments. To do this another token rule must be defined that starts with t\_ignore. The scanner will then match these like a regular token but omit them from the final token list provided to the parser. This ensures that an unintended error doesn't happen.

\subsubsection{Difficulties}
For the most part implementing the scanner went off without a hitch. However, there were several aspects of creating the regular expressions and the order in which these regular expressions were executed that created some difficulties. For example, "ENDWHILE" will be matched by the rule 
that defines "END". To get around this a lookahead was used so "END" would only match if it wasn't followed by "WHILE". Another way to do this would be to put "ENDWHILE" before "WHILE" in the regular expression but it was decided that the lookahead was more robust and easier to read. The following lines of code  shows a regular expression that will match "END" but not "ENDWHILE".
\begin{lstlisting}
	(r"END(?!WHILE)")
\end{lstlisting}

Another tricky part was finding a way of excluding the comments from the output file. It was finally determined that a prefix of t\_error was needed to ignore the comments within the parser. This took a while as it was not realized that PLY would accept multiple t\_error rules and it had already been used to ignore whitespace.
 
This scanner had to be modified multiple times to make it work better with the parser. The changes that were applied
will be discussed in full there but in general the scanner was not specific enough and needed more token types to effectively check that the program had correct grammar.
		
		
		
\subsection{Parser}
\subsubsection{Tools}
The parser tool yacc, provided with PLY, was used to create the parser because PLY was used to make the scanner. This was important because yacc is designed to take the output from lex as input. Once again this helped streamline the process of creating a functional compiler.
The guide provided with ply was used to create the bare bones of the code.

\subsubsection{Implementation}
Implementing the parser with the help of yacc was a pretty strait forward task. Luckily the grammar for Micro was provided in a file and  for the most part these rules
could be copy and pasted into yacc with the correct syntax. The following lines of code from the parser illustrate how a rule is defined in yacc.
\begin{lstlisting}
def p_assign_expr(p):
    'assign_expr : id EQ_EQ expr'
\end{lstlisting}

While most of the grammar rules could be copy and pasted, some rules had to have some work done on them
to make them work with our scanners output. The most obvious were the operators. In the grammar file the operators were the literal operators such as + or *. To make these work changes the operators were given names such as PLUS or
MINUS. To check if an input program is valid the parser calls the scanner and gets the token list from it. It then
has yacc parse through the tokens and verifies that the program is grammatically correct.
\begin{figure}[h!]
	\centerline{\includegraphics[width=.4\linewidth]{pics/parse_tree}}
	\caption{Parse tree of above rule}
\end{figure}

\subsubsection{Difficulties}
One of the main difficulties that was encountered was the fact that the initial scanner that was
made did not match all the tokens the grammar needed. Initially, keywords and operators were matched as
KEYWORD and OPERATOR. For the parser more specific token recognition was needed such
as differentiating a comma from a semi colon. To fix this each operator got its own token and regular expression. In addition all the keywords were were broken out and added to the token list individually.

 Another difficulty was the fact that the parser would assume a keyword was an
identifier since the identifier rule in the scanner had higher precedence than the keyword one. This is because in yacc the shortest rule is used first. To fix this a function rule was added for identifiers. A function rule is unique in that conditions other than regular expressions can be used to decide whether the token matches. In this case an extra condition that dictated that an identifier cannot match a keyword was added to fix this problem.

In addition there was a problem with mixed syntax from several examples which were impossible to figure out for the longest time. Finally it was found that some rules in the scanner started with a
capital T instead of a lower case t. It turned out that they all had to start with a lowercase t or they all had to start with an uppercase one.
\subsection{Symbol Table}
\subsubsection{Theory}
The implementation of the symbol table is unique in that an external tool such as PLY are not used. In addition the symbol table actively interacts with the parser in that the parser calls the symbol table to add variables and scopes. The symbol table is essentially a stack of of python dictionaries with each dictionary representing a different scope. The first scope on the stack is the global scope and is available from every other scope. As functions are called new scopes are pushed onto the stack and as functions return the top scope is popped of. As variables are declared they are added to the current scopes dictionary. If a value is already in the current scope or the global scope it is an illegal declaration and an error results.

In addition there are special scopes called blocks. These result in a new scope being added to the stack when an if or loop statement is called. These scopes are unique in that variables declared in the block cannot be in the most recent function scope or the global scope. These scopes are popped off the stack when the block ends so they cannot be used after the loop or if statement ends.
\subsubsection{Implementation}
While the symbol table seems pretty straightforward in theory, in practice it proved much more difficult. The symbol table was designed as a separate python class that contains a stack of python dictionaries and various methods for adding variables and scopes to the stack correctly. An instance of this class was created in the parser and, as the rules that corresponded to adding scopes or variables were executed, the corresponding methods were called to manipulate the stack in the correct way. 

To do this the tokens had to be percolated up through the grammar. Though the exact details of how yacc does this were never fully understood, a functional solution was found.  The functions that define the grammar take in an input P. This P corresponds to the grammar rules that make up that grammar rule. To better illustrate this reference this grammar function.
\begin{lstlisting}
def p_id_list(p):
	'id_list : id id_tail'
	p[0] = [p[1]] + p[2]
	
def p_id(p):
	'id : IDENTIFIER'
	p[0] = p[1]
\end{lstlisting}
It can be seen that p\_id takes in p where p is a list. p[0] is the value that will be percolated up through the grammar and corresponds to the left side of the grammar rules. The rest of the slots in the list correspond to what rules make up this rule. So in this case by making p[0] = to p[1], when this id is used its value will be the value of the token IDENTIFIER. In addition if that id is used by p\_id\_list the slot p[1] will be will now have the value of the IDENTIFIER. 

If p\_id\_list is in another grammar rule, it will be composed of its id and id\_tail in a list. In this way the values that need to be added to the symbol table can be added when the scopes are declared such as in an if statement. 
\begin{lstlisting}
def p_if_stmt(p):
	'if_stmt : s_if L_PAR cond R_PAR decl stmt_list else_part ENDIF'
	table.block_end()
\end{lstlisting}
	Here it can be seen that if rules are passed up in this way the variables that are being declared can be accessed by checking p[5] for the variables that need to be added to the scope of the block.


\subsubsection{Difficulties}
One of the main difficulties was figuring out where to call functions from the symbol table class in the parser. Since the parser is an LR-parser the functions in the parser are actually called in reverse. So an IDENTIFIER is set to an id before it is known what it is for. This meant that the scope starts and stops were in reverse order. For example, an if grammar function will execute before the grammar function that defines a function will execute. To remedy this some new grammar rules were added. Here is one.

\begin{lstlisting}
def p_s_if(p):
	's_if : IF'
	table.block_start()
\end{lstlisting}
	Here the token IF was taken out of the p\_if\_stmnt and placed in a new rule. Since this rule is a level below the if rule it executes before the variables are declared for it. This calls the block\_start function of the symbol table which adds a scope to the stack before variables are declared.
	
	\begin{figure}[h!]
		\centerline{\includegraphics[width=.4\linewidth]{pics/sybol_output}}
		\caption{sample output of symbol table}
	\end{figure}
	
\subsection{Semantic Routines}
\subsubsection{Implementation}
The semantic routines part of the compiler consists of two steps. The first is to convert the source code into an intermediate representation(IR) of assembly using the parsers information. The purpose of this IR is to make converting the code easier and to make the compiler more modular. This way the IR can be converted into a number of different platform specific assembly instructions.

The process of extracting the IR from the parser is similar to what was done in the symbol table. This time assemble instructions need to passed up through the grammar.  The semantic routines was designed as a separate python class that consists of several methods that take information from the parser and use it to create IR instructions using semantic actions.\cite{book} In this case the semantic action is the act of identifying the grammar rules that need to include semantic actions. In general this includes all assignments, conditionals, and outputs. These instructions are then percolated to the root of the grammar using the same technique as the symbol table, but to a greater extent. The following is an example of one of the methods from the SemanticRoutine class being called in the parser.
\begin{lstlisting}
def p_assign_expr(p):
	'assign_expr : id EQ_EQ expr'
	p[0] = routine.add_assign_expr(p[3], p[1])
\end{lstlisting}
This shows the routine taking in the id and expression and returning to p[0] to be percolated back up through the grammar. The IR code returned would be something like 
\begin{lstlisting}
STORE expr, id
\end{lstlisting}
\begin{figure}[h!]
	\centerline{\includegraphics[width=.4\linewidth]{pics/ir}}
	\caption{IR output}
\end{figure}

The second part consisted of converting the produced IR code into the tiny assembly language. This step was pretty simple for the most part. A class was made that takes the completed IR code in as input and steps through it line by line converting each instruction to tiny. The class has a method for each instruction and uses the respective methods to make the tiny code. 
\begin{figure}[h!]
	\centerline{\includegraphics[width=.4\linewidth]{pics/tiny}}
	\caption{Final tiny output}
\end{figure}
\subsubsection{Difficulties}
	One difficulty with converting the code to IR was figuring out how to make the code percolate up through the parser in a meaningful way and keeping track of registers correctly. It took in depth study of the grammar to figure out what needed to have a semantic routine in it. In addition it was hard to keep concatenating the list in the correct order to get the correct IR.
	
	A difficulty that was had while making the IR to tiny converter was handling registers. Since the IR was 3 address code and tiny is 2 address code some instructions had to be split into multiple instructions with a place holding register. In addition the tiny code can only have one var as input so if two vars are in an instruction it has to be further split up with a new register. To remedy this a register offset counter was made so each time a new register had to be used the offset would increment. Then for the rest of the code registers would added to tiny as its register number plus the register offset.

\subsection{Full-fledged Compiler}
	\begin{figure}[h!]
		\centerline{\includegraphics[width=.4\linewidth]{pics/uml}}
		\caption{Diagram of final compiler}
	\end{figure}
	
The whole compiler is combined in the parser file. The parser file has an instance of the scanner, symbol table, semantic routines, and IR to tiny. This was done because all of the other parts rely heavily on the parser except for the scanner. Since methods from all of these classes were called at specific times in the parser it only made sense to have an instance in the parser.

One good design decision was to split up each section into its own file. At first it seemed like it might be easier to implement the scanner and parser in the same file but it was decided to split them up. This was invaluable as sections were added because it could have gotten bloated and confusing. One difference that might of made sense was to exclude the IR to tiny class from the parser and had that be called directly from the semantic routine class.

Another good decision was deciding to use git and github for version control and sharing of the code. This was extremely helpful as everyone could work on the code at the same time and it made it easier to reuse old code.
\section{Conclusion and Future Work}
The result of all this was a fully functional compiler. It successfully creates a token list from the scanner, uses a parser to decide if the program has correct grammar, creates a symbol table to show declaration and assignment, and finally uses semantic routines to convert the program into the tiny language. Even though this compiler works there are several improvements that could be made.

One main limitation of this compiler is that it currently only supports one main function. Additional semantic actions will be needed to make it support multiple functions. Another limitation is the fact that errors do not give line numbers or give any information other than what type of error it is. In the future it would be nice to print the whole symbol table if there is an error with scope so one could more easily figure out what went wrong.

There is still much work that could be done on this compiler project in the future. One of the many things that could be done is to run some optimizations on the intermediate code that was generated as the output of the parser and semantic routines.
Eliminating common sub-expressions would be a good place to start since it would be a relatively simple idea to begin implementing. The result of performing common sub-expression elimination would be not only simpler code but also more efficient code because values are not needlessly recomputed but instead computed just once and then referenced every in every other location the same calculation was being made.

Another area of future work would be to improve register allocation. Currently the compiler is not very limited in the required number of registers that it can use. Once a register has been used the compiler will move to the next register and never reuse a previously used one.

This could be taken care or by implementing some sort of flag that would tell the compiler whether it is safe to write in a register or if it is dirty. The next step in optimizing register allocation would be to limit the number of registers to just a few and manage which values can be loaded into a register and which registers should be written to memory for further use later in the program. This would require the program to calculate the liveness of each variable and then perform an algorithm, such as bottom-up register allocation or top-down register allocation, to allocate available registers.

Improving the selected instructions for each operation could be done as well which goes along with peep-hole optimizations to remove redundant instructions that do not efficiently use the computational time of a computer.


\begin{flushleft}
	\bibliographystyle{unsrt}
	\bibliography{sources}
\end{flushleft}




\end{document}  